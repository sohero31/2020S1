{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSQVr3b7v_Rg"
   },
   "source": [
    "# An End-to-End Text Classification System\n",
    "\n",
    "In this workshop you will implement a text classification system from scratch. This means that we will not rely on Keras' convenient data sets. These data sets are pre-processed and it will be useful if you know how to tokenise and find the word indices of text collections not provided by Keras.\n",
    "\n",
    "The task will be to classify questions. To run this task we advice that you use [Google Colaboratory](https://colab.research.google.com) (also called Google Colab), which is a cloud solution to run Jupyter notebooks. The demonstrator will show how to run Jupyter notebooks. For additional information and to practice with the use of notebooks in Google Colab, you can also follow this link:\n",
    "\n",
    "* [Welcome notebook and link to additional resources](https://colab.research.google.com/notebooks/welcome.ipynb)\n",
    "\n",
    "## Question Classification\n",
    "\n",
    "NLTK has a corpus of questions and their question types according to a particular classification scheme (e.g. DESC refers to a question expecting a descriptive answer, such as one starting with \"How\"; HUM refers to a question expecting an answer referring to a human). Here's some example of use of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "HdoJ387Vv_Rq",
    "outputId": "898327fe-98d1-46b2-f003-2d82149b474f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/qc.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"qc\")\n",
    "from nltk.corpus import qc\n",
    "train = qc.tuples(\"train.txt\")\n",
    "test = qc.tuples(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "U78gbdkzv_SM",
    "outputId": "fe8d9865-b5d7-4716-bbae-cde13050e644"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DESC:manner', 'How did serfdom develop in and then leave Russia ?'),\n",
       " ('ENTY:cremat', 'What films featured the character Popeye Doyle ?'),\n",
       " ('DESC:manner', \"How can I find a list of celebrities ' real names ?\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "dUCCU6awv_Si",
    "outputId": "e46ecb4c-61ae-4214-9d7f-6de2559930d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NUM:dist', 'How far is it from Denver to Aspen ?'),\n",
       " ('LOC:city', 'What county is Modesto , California in ?'),\n",
       " ('HUM:desc', 'Who was Galileo ?')]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCOTaE2Yv_S-"
   },
   "source": [
    "### Exercise: Find all question types\n",
    "Write Python code that lists all the possible question types of the training set (**remember: for data exploration, never look at the test set**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yn7EUUwUv_TE"
   },
   "outputs": [],
   "source": [
    "qtypes = # ... write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "id": "AvETLOhxv_TW",
    "outputId": "cc804643-b210-4622-8dd6-6e910f21a5ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NUM:volsize',\n",
       " 'LOC:country',\n",
       " 'NUM:money',\n",
       " 'HUM:desc',\n",
       " 'HUM:title',\n",
       " 'NUM:perc',\n",
       " 'ENTY:cremat',\n",
       " 'ENTY:body',\n",
       " 'ENTY:substance',\n",
       " 'ENTY:animal',\n",
       " 'ENTY:symbol',\n",
       " 'ENTY:product',\n",
       " 'NUM:temp',\n",
       " 'ENTY:word',\n",
       " 'ENTY:techmeth',\n",
       " 'DESC:reason',\n",
       " 'ENTY:sport',\n",
       " 'ABBR:exp',\n",
       " 'ENTY:lang',\n",
       " 'LOC:city',\n",
       " 'NUM:date',\n",
       " 'HUM:ind',\n",
       " 'ENTY:event',\n",
       " 'ENTY:plant',\n",
       " 'LOC:mount',\n",
       " 'ENTY:letter',\n",
       " 'NUM:other',\n",
       " 'NUM:ord',\n",
       " 'ENTY:other',\n",
       " 'ENTY:religion',\n",
       " 'ENTY:dismed',\n",
       " 'ENTY:instru',\n",
       " 'NUM:count',\n",
       " 'ENTY:food',\n",
       " 'DESC:desc',\n",
       " 'NUM:speed',\n",
       " 'NUM:dist',\n",
       " 'ENTY:veh',\n",
       " 'NUM:weight',\n",
       " 'ENTY:currency',\n",
       " 'NUM:code',\n",
       " 'LOC:other',\n",
       " 'NUM:period',\n",
       " 'DESC:def',\n",
       " 'LOC:state',\n",
       " 'HUM:gr',\n",
       " 'ENTY:color',\n",
       " 'ENTY:termeq',\n",
       " 'ABBR:abb',\n",
       " 'DESC:manner']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inQh0Nrjv_Tp"
   },
   "source": [
    "### Exercise: Find all general types\n",
    "\n",
    "The question types have two parts. The first part describes a general type, and the second part defines a subtype. For example, the question type `DESC:manner` belongs to the general `DESC` type and within that type to the `manner` subtype. Let's focus on the general types only. Write Python code that lists all the possible general types (there are 6 of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C4pjpWV9v_Tt",
    "outputId": "cc7df776-c9df-412e-b77a-88a93ba50f6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENTY', 'ABBR', 'LOC', 'DESC', 'HUM', 'NUM']"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_types = # ... write your answer here\n",
    "general_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnMXsRBMv_T-"
   },
   "source": [
    "### Exercise: Partition the data\n",
    "There is a train and test data, but for this exercise we want to have a partition into train, dev-test, and test. In this exercise, combine all data into one array and do a 3-way partition into train, dev-test, and test. Make sure that you shuffle the data prior to doing the partition. Also, make sure that you only use the general label types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "qq6JJ-EBv_UE",
    "outputId": "66d0661d-191c-4cf4-fe4c-087b194d5e8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DESC', 'How did serfdom develop in and then leave Russia ?'),\n",
       " ('ENTY', 'What films featured the character Popeye Doyle ?')]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdata = # ... write your answer here\n",
    "qdata[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWQx8J2tv_Ua"
   },
   "outputs": [],
   "source": [
    "# Write your code here for the partition of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iri6AvfYv_Uj"
   },
   "source": [
    "### Exercise: Tokenise the data\n",
    "\n",
    "Use Keras' tokeniser to tokenise all the data. For this exercise we will use only the 100 most frequent words in the training set (since you aren't supposed to use the dev-test or test sets to extract features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Jbf0L_Qv_Un"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UE2ecEF2v_U2"
   },
   "outputs": [],
   "source": [
    "indices_train = # ... write your code here\n",
    "indices_devtest = # ... write your code here\n",
    "indices_test = # ... write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epuXbJoyv_VA"
   },
   "source": [
    "### Exercise: Vectorize the data\n",
    "The following code shows the distribution of lengths of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "ZEaD0ouFv_VE",
    "outputId": "ebc05931-72ee-450d-8d78-bd5602d3fc53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  43., 1001., 1327.,  815.,  169.,  162.,   43.,    7.,    2.,\n",
       "           2.]),\n",
       " array([ 0. ,  1.8,  3.6,  5.4,  7.2,  9. , 10.8, 12.6, 14.4, 16.2, 18. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+pLUEUNfkjryvxRazmHq32D/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist([len(d) for d in indices_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t34OuB9vv_VQ"
   },
   "source": [
    "The histogram shows that the longest question in the training data has 18 word indices, but by far most of the questions have at least 10. Based on this, use Keras' `pad_sequences` to vectorize the questions into sequences of 10 word indices. The default will be to truncate the beginning, but we want to truncate the end (since the first words of a question are often very important to determine the question type). For this you can use the option `truncating='post'`: https://keras.io/preprocessing/sequence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15jgaUwpv_VS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 10\n",
    "x_train = pad_sequences(indices_train, maxlen=maxlen, truncating='post')\n",
    "x_devtest = pad_sequences(indices_devtest, maxlen=maxlen, truncating='post')\n",
    "x_test = pad_sequences(indices_test, maxlen=maxlen, truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qn1vV1Kov_Vd"
   },
   "source": [
    "### Exercise: Vectorise the labels\n",
    "Convert the labels to one-hot encoding. If you use Keras' `to_categorical` you would first need to convert the labels to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OfRBWqadv_Vh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwNCab0Kv_Vs"
   },
   "source": [
    "### Exercise: Define the model\n",
    "\n",
    "Define a model for classification. For this model, use a feedforward architecture with an embedding layer of size 20, a layer that computes the average of word embeddings (use `GlobalAveragePooling1D`), a hidden layer of 16 units, and `relu` activation. You need to determine the size and activation of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "pBcT2-btv_Vu",
    "outputId": "a53ea9fe-1e35-49f5-c8f6-5b34e06767ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 20)            2000      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 102       \n",
      "=================================================================\n",
      "Total params: 2,438\n",
      "Trainable params: 2,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
    "\n",
    "embedding_dim = 20\n",
    "\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQxpGsUcv_V5"
   },
   "source": [
    "### Exercise: Train and evaluate\n",
    "Train your model. In the process you need to determine the optimal number of epochs. Then answer the following questions:\n",
    "1. What was the optimal number of epochs and how did you determine this?\n",
    "2. Is the system overfitting? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8DfIKpyYv_V7",
    "outputId": "4aa48185-a265-46b0-b9bd-b2872662ab15"
   },
   "outputs": [],
   "source": [
    "# Write your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbK8NkhRv_Wb"
   },
   "source": [
    "### Optional Exercise: Data exploration\n",
    "Plot the distribution of labels in the training data and compare with the distribution of labels in the devtest data. Plot also the distribution of predictions in the devtest data. What can you learn from this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "ZYWeFIznv_Wg",
    "outputId": "8c2291ac-a1ab-4d40-aa89-9b902c15b312"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOUTCrdqv_Xi"
   },
   "source": [
    "### Optional Exercise: Improve your system\n",
    "\n",
    "Try the following options:\n",
    "\n",
    "1. Use pre-trained word embeddings\n",
    "2. Use recurrent neural networks.\n",
    "\n",
    "Feel free to try each option separately and in combination, and compare the results. Feel also free to try with other variants of the initial architecture, such as:\n",
    "\n",
    "1. Introducing more hidden layers.\n",
    "2. Changing the size of embeddings.\n",
    "3. Changing the number of units in the hidden layer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Q-1GtYOv_Xl"
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "W05 Workshop-Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
